# -*- coding: utf-8 -*-
"""Generative_Project_Milestone_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mco7fK5fs4gHOqds19UJfIoD7H_nVNDm

#1. Installing Libraries and extracting Dataset
"""

!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI

!pip install pycocotools

!pip install --upgrade scikit-image scipy

import os
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.colors as colors
import seaborn as sns
import numpy as np

from random import shuffle
from PIL import Image

from pycocotools.coco import COCO

"""#2. Extracting Dataset"""

# Load COCO annotation file (you can also use the test file)
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!unzip -q annotations_trainval2017.zip -d /content/drive/MyDrive/COCO_dataset/annotations

# Initialize COCO API for instance annotations
annFile = '/content/drive/MyDrive/COCO_dataset/annotations/annotations/instances_train2017.json'
coco = COCO(annFile)

from google.colab import drive
drive.mount('/content/drive')

"""I have added a cell to mount your Google Drive. Please run it and authorize Colab to access your Drive.

Now, I will modify the code in cell `E7SYS1ZFrFoo` to extract the annotations to `/content/drive/MyDrive/COCO_dataset/annotations` and update the `annFile` variable to reflect this path.

Here is the code to load COCO annotations and display the categories.

**Important:** The `annotation_path` variable needs to be set to the actual path of your COCO annotation file.
"""

import pandas as pd

ANNOT_URL = "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
ANNOT_DIR = "./annotations"
ANNOT_FILE = f"{ANNOT_DIR}/annotations/instances_train2017.json"

if not os.path.exists(ANNOT_FILE):
    !wget -q $ANNOT_URL -O annotations.zip
    !unzip -q annotations.zip -d $ANNOT_DIR

# --- Step 4: Initialize COCO API ---
coco = COCO(ANNOT_FILE)

# --- Step 5: Get all categories ---
cat_ids = coco.getCatIds()
cats = coco.loadCats(cat_ids)

# --- Step 6: For each category, count how many images belong to it ---
category_counts = []
for cat in cats:
    img_ids = coco.getImgIds(catIds=[cat['id']])
    category_counts.append({
        'Category': cat['name'],
        'Supercategory': cat['supercategory'],
        'Num_Images': len(img_ids)
    })

# --- Step 7: Display in tabular format ---
df = pd.DataFrame(category_counts).sort_values(by='Num_Images', ascending=False).reset_index(drop=True)

# Set Pandas display options to ensure tabular view
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

# --- Step 8: Display nicely ---
from IPython.display import display
display(df.style.set_properties(**{'text-align': 'center'}))

# Optional: quick summary
print(f"\nTotal Categories: {len(df)}")
unique_imgs = len(set(sum([coco.getImgIds(catIds=[c['id']]) for c in cats], [])))
print(f"Total Unique Images (approx, may overlap): {unique_imgs}")

from pycocotools.coco import COCO
import pandas as pd
import requests

# Load captions annotation file
!wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip
!unzip -q annotations_trainval2017.zip -d ./annotations

annFile = './annotations/annotations/captions_train2017.json'
coco_caps = COCO(annFile)

# Get image-caption pairs (limit to 2000 for now)
anns = coco_caps.loadAnns(coco_caps.getAnnIds())[:2000]
data = []
for a in anns:
    img = coco_caps.loadImgs(a['image_id'])[0]
    data.append({
        "image_url": img['coco_url'],
        "caption": a['caption']
    })

df = pd.DataFrame(data)
df.head()

"""#3. Model Pipeline"""

!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import torch
import clip
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

text_inputs = torch.cat([clip.tokenize(df['caption'].iloc[i]) for i in range(5)]).to(device)
with torch.no_grad():
    text_features = model.encode_text(text_inputs)
print("Embedding shape:", text_features.shape)

# --- STEP 1: Install Dependencies ---
!pip install -q diffusers==0.31.0 transformers accelerate torch torchvision safetensors

# --- STEP 2: Imports ---
from diffusers import StableDiffusionPipeline
import torch
from PIL import Image
import matplotlib.pyplot as plt

# --- STEP 3: Load Pretrained Stable Diffusion Model ---
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

# --- STEP 4: Function to Generate Room Image ---
def generate_room(room_type, size, objects_list, style="modern"):
    prompt = f"A {style} {room_type} of size {size}, decorated with {', '.join(objects_list)}, realistic lighting, 4K interior render"
    print(f"\nPrompt → {prompt}")
    image = pipe(prompt, num_inference_steps=30, guidance_scale=8.5).images[0]
    plt.imshow(image)
    plt.axis('off')
    plt.show()
    return image

# --- STEP 5: Example Run ---
generate_room(
    room_type="kitchen",
    size="10x12 ft",
    objects_list=["dining table", "refrigerator", "microwave", "plant"],
    style="cozy modern"
)

# --- Step 1: Install dependencies (run once per session) ---
!pip install -q diffusers transformers accelerate safetensors torch torchvision

# --- Step 2: Import and load model ---
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

# --- Step 3: Custom user prompt ---
prompt = input("Enter your prompt (e.g., 'A cozy 12x10 kitchen with fridge and plants'): ")

# --- Step 4: Generate and display ---
image = pipe(prompt, guidance_scale=7.5, num_inference_steps=30).images[0]
image.show()

base_prompt = input("Enter base idea (e.g., 'modern kitchen'): ")
categories = ["refrigerator", "dining table", "lamp"]
prompt = f"A {base_prompt} with {', '.join(categories)}, 4K photo"
print("Final prompt:", prompt)

image = pipe(prompt, guidance_scale=7.5, num_inference_steps=30).images[0]
from IPython.display import display
display(image)

import gradio as gr

def generate(prompt):
    image = pipe(prompt, guidance_scale=7.5, num_inference_steps=30).images[0]
    return image

gr.Interface(
    fn=generate,
    inputs=gr.Textbox(label="Describe your room"),
    outputs="image",
    title="Room Decor Generator"
).launch()

"""#4. Git Repository Setup"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/generative-ai-room-decorator/{data/raw,notebooks,src,outputs,configs}

!touch generative-ai-room-decorator/{README.md,.gitignore}

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/generative-ai-room-decorator

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/generative-ai-room-decorator
!git init
!git remote add origin git@github.com:<username>/generative-ai-room-decorator.git

!ssh-keygen -t ed25519 -C "pillalamarri.v@northeastern.edu"

# 2️⃣ Start SSH agent and add key
!eval "$(ssh-agent -s)"
!ssh-add ~/.ssh/id_ed25519

# 3️⃣ Show your public key
!cat ~/.ssh/id_ed25519.pub

!eval "$(ssh-agent -s)"    # start SSH agent
!ssh-add ~/.ssh/id_ed25519 # add your SSH key to the agent

!ssh -T git@github.com

# === START: SSH Agent and Key Setup ===

# Start SSH agent and add key in one shell session
!eval "$(ssh-agent -s)"
!ssh-add ~/.ssh/id_ed25519

# Add GitHub to known_hosts (avoids "host key verification failed")
!mkdir -p ~/.ssh
!ssh-keyscan github.com >> ~/.ssh/known_hosts

# Set correct permissions
!chmod 700 ~/.ssh
!chmod 644 ~/.ssh/known_hosts

# Test SSH connection
!ssh -T git@github.com
# === END ===

!git remote -v

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/generative-ai-room-decorator

!git rebase --abort

!git status
!git branch

!git switch -C main

!git remote remove origin
!git remote add origin git@github.com:PVRPratyusha/generative-ai-room-decorator.git
!git fetch origin

!git add .
!git commit -m "Clean branch reset after rebase issue"
!git push origin main --force

!echo ".ipynb_checkpoints/" >> .gitignore
!git add .gitignore
!git commit -m "Ignore Colab checkpoint files"
!git push origin main

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/generative-ai-room-decorator/notebooks
!cp Generative_Project_Milestone_1.ipynb Generative_Project_Milestone_1_broken_backup.ipynb

# from google.colab import files
# uploaded = files.upload()

!mv "/content/Generative_Project_Milestone_1.ipynb" "/content/drive/MyDrive/generative-ai-room-decorator/notebooks/"

!ls /content

!ls "/content/drive/MyDrive/generative-ai-room-decorator/notebooks"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/generative-ai-room-decorator
!git add notebooks/Generative_Project_Milestone_1.ipynb
!git commit -m "Re-upload working Milestone 1 notebook"
!git push origin main

